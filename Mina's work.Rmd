---
title: "mina's project"
author: "Mina Mehdinia"
date: "2023-03-03"
output: html_document
---


```{r setup, include = F}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(MASS)        # LDA, QDA, OLS, Ridge Regression, Box-Cox, stepAIC, etc...
library(ROCR)        # Precision/Recall/Sens./Spec./ performance plot
library(class)       # KNN, SOM, LVQ
library(e1071)       # Naive Bayesian Classifier, SVM, GKNN, ICA, LCA
library(boot)        # LOOCV, Bootstrap
library(caret)       # Classification/Regression Training
library(dplyr)       # For piping and mutation methods
library(ggplot2)     # For plots
library(randomForest)# for RFE
library(naivebayes)  # for NB
library(ROSE)        # for SMOTE
library(fastDummies) # For Get Dummies
library(pROC)        # for PCA
```

```{r}
#LDA
german <- read.csv("germancredit.csv", header=T)
head(german)
```

```{r}
german[, sapply(german, is.character)] <- lapply(german[, sapply(german, is.character)], as.factor)

german$Default = as.factor(german$Default)

train_index = sample(nrow(german), nrow(german) * 0.5) # 50% for training

set.seed(123) # for reproducibility
train_data =german[]
test_data = german[-train_index, ]
test_Y = test_data$Default
  
results.matrix = matrix(0, nrow = 5, ncol = 4)


lda.fit = lda(Default~. , data = train_data)

#plot(lda.fit)
lda.pred = predict(lda.fit, test_data)
names(lda.pred)
lda.class = lda.pred$class

table(lda.class, test_Y)
table(test_Y, lda.class)


sum(lda.pred$posterior[,1] >= 0.5) 
sum(lda.pred$posterior[,1] < 0.5)  



lda.tn = sum((test_Y == unique(test_Y)[1])&(lda.class == unique(test_Y)[1]))
lda.tp = sum((test_Y == unique(test_Y)[2])&(lda.class == unique(test_Y)[2]))

lda.fp = sum((test_Y == unique(test_Y)[1])&(lda.class == unique(test_Y)[2]))
lda.fn = sum((test_Y == unique(test_Y)[2])&(lda.class == unique(test_Y)[1]))

lda.n = lda.tn + lda.fp
lda.p = lda.fn + lda.tp
  
  

# Specificity, Sensitivity, Overall Error/ Probability of Misclassification #

spec.lda  = 1 - (lda.fp/lda.n)
sen.lda   = lda.tp/lda.p
oer.lda1  = (lda.fn + lda.fp)/(lda.n + lda.p)
#oer.lda2  = 1 - mean(lda.class == Direction.2005)



# ROCR #

lda.pred <- prediction(lda.pred$posterior[,2], test_data$Default) 
lda.perf <- performance(lda.pred,"tpr","fpr")
plot(lda.perf,colorize=TRUE, lwd = 2)
abline(a = 0, b = 1) 


lda.auc = performance(lda.pred, measure = "auc")
#print(lda.auc@y.values)

colnames(results.matrix) = c("SPEC", "SENS", "OER", "AUC")
rownames(results.matrix) = c("LDA", "QDA", "NAIVE-B", "KNN", "LOG")
results.matrix[1,] = as.numeric( c(spec.lda, sen.lda, oer.lda1, lda.auc@y.values))
results.matrix

```
```{r}
#QDA
qda.fit = qda(Default~. , data = train_data)


qda.pred  = predict(qda.fit,  test_data)
qda.class = qda.pred$class

table(qda.class, test_Y)


qda.tn = sum((test_Y == unique(test_Y)[1])&(qda.class == unique(test_Y)[1]))
qda.tp = sum((test_Y == unique(test_Y)[2])&(qda.class == unique(test_Y)[2]))

qda.fp = sum((test_Y == unique(test_Y)[1])&(qda.class == unique(test_Y)[2]))
qda.fn = sum((test_Y == unique(test_Y)[2])&(qda.class == unique(test_Y)[1]))

qda.n = qda.tn + qda.fp
qda.p = qda.fn + qda.tp



# Specificity, Sensitivity, Overall Error/ Probability of Misclassification #
spec.qda  = 1 - (qda.fp/qda.n)
sen.qda   = qda.tp/qda.p
oer.qda1  = (qda.fn + qda.fp)/(qda.n + qda.p)
#oer.qda2  = 1 - mean(qda.class == Direction.2005)



# ROCR #
qda.pred <- prediction(qda.pred$posterior[,2], test_data$Default) 
qda.perf <- performance(qda.pred,"tpr","fpr")
plot(qda.perf,colorize=TRUE, lwd = 2)
abline(a = 0, b = 1) 


qda.auc = performance(qda.pred, measure = "auc")
print(qda.auc@y.values)


results.matrix[2,] = as.numeric( c(spec.qda, sen.qda, oer.qda1, qda.auc@y.values))
results.matrix
```

```{r}
class(test_Y)
```

```{r}
test_Y
```

```{r}
# NB
nb.fit = naiveBayes(Default~. , data = train_data)

nb.class = predict(nb.fit , test_data)
table(nb.class, test_Y)

mean(nb.class == test_Y)


nb.tn = sum((test_Y == unique(test_Y)[1])&(nb.class == unique(test_Y)[1]))
nb.tp = sum((test_Y == unique(test_Y)[2])&(nb.class == unique(test_Y)[2]))

nb.fp = sum((test_Y == unique(test_Y)[1])&(nb.class == unique(test_Y)[2]))
nb.fn = sum((test_Y == unique(test_Y)[2])&(nb.class == unique(test_Y)[1]))

nb.n = nb.tn + nb.fp
nb.p = nb.fn + nb.tp




# Specificity, Sensitivity, Overall Error/ Probability of Misclassification #
spec.nb  = 1 - (nb.fp/nb.n)
sen.nb   = nb.tp/nb.p
oer.nb1  = (nb.fn + nb.fp)/(nb.n + nb.p)
#oer.nb2  = 1 - mean(nb.class == Direction.2005)


# ROCR #
nb.pred = predict(nb.fit , test_data, type = "raw")
nb.pred <- prediction(nb.pred[,2], test_data$Default) 
nb.perf <- performance(nb.pred,"tpr","fpr")
plot(nb.perf,colorize=TRUE, lwd = 2)
abline(a = 0, b = 1) 


nb.auc = performance(nb.pred, measure = "auc")
#print(nb.auc@y.values)


results.matrix[3,] = as.numeric( c(spec.nb, sen.nb, oer.nb1, nb.auc@y.values))

results.matrix
```

```{r}
# train_data.x = cbind(train_data[,-1])
# test_data.x = cbind(test_data[,-1])
# train_Y = train_data$Default
```

```{r}
# Make a dataset of 150% of the original.
# Now we can perform the industry standard train, test, split on this 150% data.
german.half.indexes <- sample(1:nrow(german), 0.5 * nrow(german))
german.half <- german[german.half.indexes, ]
german.150.percent <- rbind(german, german.half)
paste0("The dataset now has ", dim(german.150.percent)[1], " rows. ")
```

```{r}
# Make a copy
data <- data.frame(german.150.percent)

# Convert categorical variables to numerical variables using one-hot encoding
data <- model.matrix(~ . - 1, data = data)

# Split the data into training and testing sets
trainIndex <- sample(1:nrow(data), 0.7 * nrow(data))
train_data.x <- data[trainIndex, ]
test_data.x <- data[-trainIndex, ]

# Define the predictor variables and the target variable
predictors <- colnames(train_data.x)[-ncol(train_data.x)]

# Target after getting dummies (necessary for KNN)
target <- "Default1"

knn.test_Y <- test_data.x[, target]
knn.train_Y <- train_data.x[, target]

# Train the k-NN model
#model <- knn(train_data.x, test_data.x, train_Y, k = 3)
knn.fit <- class::knn(train_data.x, test_data.x, knn.train_Y, k=2, prob=TRUE)
knn.prob <- attr(knn.fit, "prob")

# these come on a form that ROCR does not accept so we need to invert them for
# the -1 class and rescale them.
knn.prob <- 2*ifelse(knn.fit == "-1", 1-knn.prob, knn.prob) - 1

# # Predict the target variable for the test set
knn.pred <- as.factor(knn.fit)
# 
# # Print the accuracy of the model
# accuracy <- mean(knn.pred == test_Y)
# cat("Accuracy:", accuracy)
```


```{r}
knn.tn = sum((knn.pred == unique(knn.test_Y)[1])&(knn.pred == unique(knn.test_Y)[1]))
knn.tp = sum((knn.pred == unique(knn.test_Y)[2])&(knn.pred == unique(knn.test_Y)[2]))

knn.fp = sum((knn.pred == unique(knn.test_Y)[1])&(knn.pred == unique(knn.test_Y)[2]))
knn.fn = sum((knn.pred == unique(knn.test_Y)[2])&(knn.pred == unique(knn.test_Y)[1]))

knn.n = knn.tn + knn.fp
knn.p = knn.fn + knn.tp

# Specificity, Sensitivity, Overall Error/ Probability of Misclassification #
spec.knn  = 1 - (knn.fp/knn.n)
sen.knn   = knn.tp/knn.p
oer.knn1  = (knn.fn + knn.fp)/(knn.n + knn.p)
#oer.knn2  = 1 - mean(knn.pred == Direction.2005)
```

```{r}
# ROCR #
knn.pred <- prediction(knn.prob, knn.test_Y)
knn.pred <- performance(knn.pred, "tpr", "fpr")
plot(knn.pred, avg= "threshold", colorize=T, lwd=3, main="KNN ROC curve")
abline(a = 0, b = 1) 
```

```{r}
knn.auc <- (knn.tp + knn.tn)/(knn.n + knn.p)
knn.auc
```

```{r}
#knn.auc = performance(knn.pred, measure = "auc")
#print(knn.auc@y.values)


results.matrix[4,] = as.numeric( c(spec.knn, sen.knn, oer.knn1, knn.auc))
results.matrix
```

```{r}
log.reg = glm(Default~. , data = train_data, family = binomial(link = "logit"))

#summary(log.reg)

log.pred = predict(log.reg, test_data, type = "response")


#contrasts(test_Y)

log.pred <- ifelse(log.pred >= 0.5, 1, 0)
  

table(log.pred, test_Y)


log.tn = sum((test_Y == unique(test_Y)[1])&(log.pred == unique(test_Y)[1]))
log.tp = sum((test_Y == unique(test_Y)[2])&(log.pred == unique(test_Y)[2]))

log.fp = sum((test_Y == unique(test_Y)[1])&(log.pred == unique(test_Y)[2]))
log.fn = sum((test_Y == unique(test_Y)[2])&(log.pred == unique(test_Y)[1]))

log.n = log.tn + log.fp
log.p = log.fn + log.tp



# Specificity, Sensitivity, Overall Error/ Probability of Misclassification #

spec.log  = 1 - (log.fp/log.n)
sen.log   = log.tp/log.p
oer.log1  = (log.fn + log.fp)/(log.n + log.p)

 

# ROCR #
log.pred <- prediction(log.probs, test_data$Default) 
log.perf <- performance(log.pred,"tpr","fpr")
plot(log.perf,colorize=TRUE, lwd = 2)
abline(a = 0, b = 1) 


log.auc = performance(log.pred, measure = "auc")

results.matrix[5,] = as.numeric( c(spec.log, sen.log, oer.log1, log.auc@y.values))
results.matrix
```
