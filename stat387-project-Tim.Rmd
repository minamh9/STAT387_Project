---
title: "Tim's Work"
author: "Tim Luo"
date: "2023-03-03"
output: html_document
---

# SETUP
```{r setup, include = F}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)   # dplyr, ggplot2, tidyr, tibble, stringr, purrr
library(MASS)        # LDA, QDA, OLS, Ridge Reg., Box-Cox, stepAIC, etc.
library(ROCR)        # Precision/Recall/Sens./Spec./ performance plot
library(class)       # KNN, SOM, LVQ
library(e1071)       # Naive Bayesian Classifier, SVM, GKNN, ICA, LCA
library(boot)        # LOOCV, Bootstrap
library(caret)       # Classification/Regression Training
library(randomForest)# for RFE
library(naivebayes)  # for NB
library(ROSE)        # for SMOTE
library(fastDummies) # For Get Dummies
library(pROC)        # display and analyze ROC curves
source("Stat387-Proj-HelperFunctions.R")
# rm(list=ls())
```

```{r}
german <- read.csv("germancredit.csv", header=T) |>
  as_tibble() |> 
  rename(checking = checkingstatus1) 

```

```{r}
dummy <- dummyVars(" ~ .", data=german, fullRank = TRUE)
newgerman <- data.frame(predict(dummy, newdata = german))
dim(newgerman)
```

## Exploratory

```{r}
german.lm = lm(Default ~ ., german)

summary(german.lm)

train = 1
test = 0.5
```

```{r grouping}
german.named = german |> 
  mutate_if(is.character, function(x) name.dict[x])

german.named |> group_by(purpose) |> 
  summarise(count = n(),
            default = sum(Default),
            percent.default = default/count) |> 
  arrange(percent.default)


german.named |> group_by(duration) |> 
  summarise(count = n(),
            default = sum(Default),
            percent.default = default/count) |> 
  arrange(desc(percent.default)) |> 
  filter(count > 9)

german.named$duration |> unique() |> sort()
```

```{r}
qda_comb_result1
qda_comb_result2
```


# FUNCTIONS

## RFE

```{r}

RFE <- function(data, num.features=4) {
  
  # Define the predictor and response variables
  train.X <- data[, !(names(data) %in% c("Default"))]
  train.Y <- as.factor(data[, "Default"])
  
  # Define the control parameters for feature selection
  ctrl <- rfeControl(functions = rfFuncs,
                     method = "cv",
                     number = 10)
  
  # Perform recursive feature elimination using the random forest algorithm
  rf_rfe <- rfe(train.X, train.Y, sizes = c(1:num.features), rfeControl = ctrl)
  
  # Print the results
  print(rf_rfe)
  
  # Plot the results
  p <- plot(rf_rfe, type = c("g", "o"))
  print(p)
  
  # Get the features
  features <- row.names(varImp(rf_rfe))[1:num.features]
  
  varimp_data <- data.frame(feature = features,
                            importance = varImp(rf_rfe)[1:num.features, 1])
  
  # Plots the variable importances
  gg <- ggplot(data = varimp_data, 
               aes(x = reorder(feature, -importance), y = importance, fill = feature)) +
    geom_bar(stat="identity") + labs(x = "Features", y = "Variable Importance") + 
    geom_text(aes(label = round(importance, 2)), vjust=1.6, color="white", size=4) + 
    theme_bw() + theme(legend.position = "none") + theme(axis.text.x = element_text(angle = 90, vjust = 0.5))
  
  print(gg)
  
  target.features <- c("Default", features)
  
  return(subset(data, select=target.features))
}

```

## PCA

```{r}

Get.First.n.PCs <- function(data, n=4) {
  # Extract the first four principal components
  first_n_pcs <- data$x[, 1:n]
  
  # Create a new PCA results object with only the first four principal components
  german.pca.first.n <- data
  german.pca.first.n$x <- first_n_pcs
  german.pca.first.n$rotation <- data$rotation[, 1:n, drop = FALSE]
  
  return(german.pca.first.n)
}

```

## LDA

```{r}
LDA.Model <- function(data, train = 1, test = 0.5) {
  # Split the data into training and testing sets
  set.seed(123) # for reproducibility
  train_index <- sample(nrow(data), nrow(data) * train)
  train_data <- data[train_index, ]
  train_label <- as.factor(data[train_index,"Default"])
  if(train == 1) {
    test_index = sample(nrow(data), nrow(data) * test)
    test_data = data[test_index, ]
    test_label = as.factor(data[test_index, "Default"])
  }
  else {
    test_data <- data[-train_index, ]
    test_label <- as.factor(data[-train_index,"Default"])
  }
  
  # Run LDA and predictions
  germanlda_short <- lda(Default ~ . , data.frame(train_data))
  predictions_short <- predict(germanlda_short, newdata = data.frame(test_data))
  
  # Printing the confusion matrix
  cm <- confusionMatrix(predictions_short$class, test_label)
  table_cm <- as.table(cm)
  acc <- sum(diag(table_cm)) / sum(table_cm)
  acc

  
  # Printing the ROC curve
  roc.curve <- roc(test_label, predictions_short$posterior[,2])
  print(plot(roc.curve))
  print(roc.curve$auc)
  
  ldaPredictiction <- prediction(
    as.numeric(predictions_short$class),
    as.numeric(test_label)
  )
  
  ldaPerf <- performance(ldaPredictiction, measure = "tpr", x.measure = "fpr")
  ldaAUC <- performance(ldaPredictiction, measure = "auc")
  
  print(plot(ldaPerf))
  
  # Report the model performance metrics for the optimal K
  # Extract performance metrics
  specificity <- slot(ldaPerf, "y.values")[[1]]
  sensitivity <- 1 - slot(ldaPerf, "x.values")[[1]]
  auc <- slot(ldaAUC, "y.values")
  
  ldaError <- mean(as.numeric(predictions_short$class) !=as.numeric(test_label))
  
  # Print performance metrics
  print(cm)
  print(paste0("AUC: ", auc))
  print(paste0("Error rate:", ldaError))
  print(paste0("specificity: ",specificity))
  print(paste0("sensitivity: ",sensitivity))
  
  # Calculate false positives
  false_positives <- sum(as.numeric(predictions_short$class) == 2 & as.numeric(test_label) == 1)
  
  # Calculate false positives as a percentage
  total_negatives <- sum(as.numeric(test_label) == 1)
  false_positives_percent <- false_positives / total_negatives * 100
  
  # Print the false positives percentage
  print(paste0("False positives percentage: ", round(false_positives_percent,3), "%"))
  
  return(list(false_positives_percent,round(acc,2)))
}

```

## QDA

```{r}
QDA.Model <- function(data, train = 1, test = 0.5) {
  # Split the data into training and testing sets
  set.seed(123) # for reproducibility
  train_index <- sample(nrow(data), nrow(data) * train) # 70% for training
  train_data <- data[train_index, ]
  train_label <- as.factor(data[train_index,"Default"])
  if(train == 1) {
    test_index = sample(nrow(data), nrow(data) * test)
    test_data = data[test_index, ]
    test_label = as.factor(data[test_index, "Default"])
  }
  else {
    test_data <- data[-train_index, ]
    test_label <- as.factor(data[-train_index,"Default"])
  }
  
 
  germanQda <- qda(Default ~ . , data.frame(train_data))
  predictions <- predict(germanQda, newdata =  data.frame(test_data))
  train_data$Default <- factor(train_data$Default, levels = levels(predictions$class))
  
  # Printing the confusion matrix
  cm <- confusionMatrix(predictions$class, test_label)
  table_cm <- as.table(cm)
  acc <- sum(diag(table_cm)) / sum(table_cm)
  

  
  # Printing the ROC curve
  roc.curve <- roc(test_label, predictions$posterior[,2])
  print(plot(roc.curve))
  print(roc.curve$auc)
  
  qdaPredictiction <- prediction(
    as.numeric(predictions$class),
    as.numeric(test_label)
  )
  
  qdaPerf <- performance(qdaPredictiction, measure = "tpr", x.measure = "fpr")
  qdaAUC <- performance(qdaPredictiction, measure = "auc")
  
  print(plot(qdaPerf))
  
  # Report the model performance metrics for the optimal K
  # Extract performance metrics
  specificity <- slot(qdaPerf, "y.values")[[1]]
  sensitivity <- 1 - slot(qdaPerf, "x.values")[[1]]
  auc <- slot(qdaAUC, "y.values")
  
  qdaError <- mean(as.numeric(predictions$class) !=as.numeric(test_label))
  
  # Print performance metrics
  print(cm)
  print(paste0("AUC: ", auc))
  print(paste0("Error rate:", qdaError))
  print(paste0("specificity: ",specificity))
  print(paste0("sensitivity: ",sensitivity))
  
  # Calculate false positives
  false_positives <- sum(as.numeric(predictions$class) == 2 & as.numeric(test_label) == 1)
  
  # Calculate false positives as a percentage
  total_negatives <- sum(as.numeric(test_label) == 1)
  false_positives_percent <- false_positives / total_negatives * 100
  
  # Print the false positives percentage
  print(paste0("False positives percentage: ", round(false_positives_percent,3), "%"))
  
  return(list(false_positives_percent,round(acc,2)))
}
```

## Logistic Regression

```{r}
GLM.Model <- function(data, train = 1, test = 0.5) {
  
  # Split the data into training and testing sets on PCA
  set.seed(123) # for reproducibility
  train_index <- sample(nrow(data), nrow(data) * train) # 70% for training
  train_data <- data[train_index, ]
  if(train == 1) {
    test_index = sample(nrow(data), nrow(data) * test)
    test_data = data[test_index, ]
    test_label = as.factor(data[test_index, "Default"])
  }
  else {
    test_data <- data[-train_index, ]
    test_label <- as.factor(data[-train_index,"Default"])
  }  
  
  # Train the GLM model using the training data
  glm_model <- glm(Default ~ ., data = train_data, family = binomial(link = "logit"))
  
  # Evaluate the model performance on the test set
  glm_pred <- predict(glm_model, newdata = test_data, type = "response")
  glm_pred <- ifelse(glm_pred >= 0.5, 1, 0)
  
  glmConf <- confusionMatrix(factor(glm_pred, levels = c(0,1)), factor(test_data$Default, levels = c(0,1)))
  table_cm <- as.table(glmConf)
  acc <- sum(diag(table_cm)) / sum(table_cm)
  
  # Get predictions
  pred_obj <- prediction(glm_pred,test_label)
  
  # Plot ROC Curve
  perf <- performance(pred_obj, measure = "tpr", x.measure = "fpr")
  plot(perf, main = "ROC Curve", col = "blue", lwd = 2, legacy.axes = TRUE)
  abline(a = 0, b = 1, lty = 2, col = "red")
  
  glmPredictiction <- prediction(as.numeric(glm_pred), as.numeric(test_label))
  glmPerf <- performance(glmPredictiction, measure = "tpr", x.measure = "fpr")
  glmAUC <- performance(glmPredictiction, measure = "auc")
  
  print(plot(glmPerf))
  
  # Extract performance metrics
  sensitivity <- slot(glmPerf, "y.values")[[1]]
  specificity <- 1 - slot(glmPerf, "x.values")[[1]]
  auc <- slot(glmAUC, "y.values")
  glmError <- mean(as.numeric(glm_pred) !=as.numeric(test_label))
  
  # Print performance metrics
  print(glmConf)
  print(paste0("Sensitivity: ", sensitivity))
  print(paste0("Specificity: ", specificity))
  print(paste0("AUC: ", auc))
  print(paste0("Error rate:", glmError))
  
  # Calculate false positives
  false_positives <- sum(as.numeric(glm_pred) == 1 & as.numeric(test_label) == 0)
  
  # Calculate false positives as a percentage
  total_negatives <- sum(as.numeric(test_label) == 0)
  false_positives_percent <- false_positives / total_negatives * 100
  
  # Print the false positives percentage
  print(paste0("False positives percentage: ", round(false_positives_percent,3), "%"))
  
  return(list(false_positives_percent, round(acc,2)))
}


```

## KNN

```{r}

# This function takes in non PCA like data.
KNN.Model <- function(data, train = 1, test = 0.5) {
  
    # Split the data into training and testing sets on PCA
  set.seed(123) # for reproducibility
  train_index <- sample(nrow(data), nrow(data) * train) # 70% for training
  train_data <- data[train_index, ]
  train_label <- as.factor(data[train_index,"Default"])
  if(train == 1) {
    test_index = sample(nrow(data), nrow(data) * test)
    test_data = data[test_index, ]
    test_label = as.factor(data[test_index, "Default"])
  }
  else {
    test_data <- data[-train_index, ]
    test_label <- as.factor(data[-train_index,"Default"])
  }
  
  # Train the KNN classifier using the training data
  knn_model <- train(
    x = train_data,
    y = train_label,
    method = "knn",
    trControl = trainControl(method = "cv", number = 10),
    tuneGrid = data.frame(k = 1:30)
  )
  
  # Get predictions
  pred <- predict(knn_model, newdata = test_data, type = "prob")
  pred_obj <- prediction(pred[,2], test_label)
  
  # Evaluate the model performance on the test set for each value of K
  knnPred <- predict(knn_model, newdata=test_data)
  knnConf <- confusionMatrix(knnPred, test_label)
  table_cm <- as.table(knnConf)
  acc <- sum(diag(table_cm)) / sum(table_cm)
  
  # Plot ROC Curve
  perf <- performance(pred_obj, measure = "tpr", x.measure = "fpr")
  plot(perf, main = "ROC Curve", col = "blue", lwd = 2, legacy.axes = TRUE)
  abline(a = 0, b = 1, lty = 2, col = "red")
  
  # Choose the K that gives the lowest test error rate
  kOpt <- knn_model$bestTune$k
  
  # Plot the tuning parameter performance
  gg <- ggplot(knn_model$results, aes(x=k, y=Accuracy)) +
    geom_line() +
    geom_point(size = 3) +
    geom_vline(xintercept=kOpt, color="red", linetype="dashed") +
    labs(title="Tuning Parameter Performance",
         x="K",
         y="Accuracy") +
    theme_minimal()
  
  print(gg)
  
  knnPredictiction <- prediction(as.numeric(knnPred), as.numeric(test_label))
  knnPerf <- performance(knnPredictiction, measure = "tpr", x.measure = "fpr")
  knnAUC <- performance(knnPredictiction, measure = "auc")
  
  print(plot(knnPerf))
  
  # Report the model performance metrics for the optimal K
  # Extract performance metrics
  sensitivity <- slot(knnPerf, "y.values")[[1]]
  specificity <- 1 - slot(knnPerf, "x.values")[[1]]
  auc <- slot(knnAUC, "y.values")
  knnError <- mean(as.numeric(knnPred) !=as.numeric(test_label))
  
  # Print performance metrics
  print(knnConf)
  print(paste0("Sensitivity: ", sensitivity))
  print(paste0("Specificity: ", specificity))
  print(paste0("AUC: ", auc))
  print(paste0("Optimal K:", kOpt))
  print(paste0("Error rate:", knnError))
  
  # Calculate false positives
  false_positives <- sum(as.numeric(knnPred) == 2 & as.numeric(test_label) == 1)
  
  # Calculate false positives as a percentage
  total_negatives <- sum(as.numeric(test_label) == 1)
  false_positives_percent <- false_positives / total_negatives * 100
  
  # Print the false positives percentage
  print(paste0("False positives percentage: ", round(false_positives_percent,3), "%"))
  
  return(list(false_positives_percent, round(acc,2)))
}

```

## Naïve Bayes

```{r}
NB.Model <- function(data, train = 1, test = 0.5) {
  
  # Split the data into training and testing sets on PCA
  set.seed(123) # for reproducibility
  train_index <- sample(nrow(data), nrow(data) * train) # 70% for training
  train_data <- data[train_index, ]
  train_label <- as.factor(data[train_index,"Default"])
  if(train == 1) {
    test_index = sample(nrow(data), nrow(data) * test)
    test_data = data[test_index, ]
    test_label = as.factor(data[test_index, "Default"])
  }
  else {
    test_data <- data[-train_index, ]
    test_label <- as.factor(data[-train_index,"Default"])
  }
  
  # Train the Naïve Bayes classifier using the training data
  nb_model <- naive_bayes(train_data, train_label)
  
  # # Evaluate the model performance on the test set
  nb_pred <- predict(nb_model, newdata=test_data)
  nbConf <- confusionMatrix(nb_pred, test_label)
  
  # Get predictions
  pred <- predict(nb_model, newdata = test_data, type = "prob")
  pred_obj <- prediction(pred[,2], test_label)
  
  # Evaluate the model performance on the test set for each value of K
  nbPred <- predict(nb_model, newdata=test_data)
  nbConf <- confusionMatrix(nbPred, test_label)
  table_cm <- as.table(nbConf)
  acc <- sum(diag(table_cm)) / sum(table_cm)
  
  # Plot ROC Curve
  perf <- performance(pred_obj, measure = "tpr", x.measure = "fpr")
  plot(perf, main = "ROC Curve", col = "blue", lwd = 2, legacy.axes = TRUE)
  abline(a = 0, b = 1, lty = 2, col = "red")
  
  nbPredictiction <- prediction(as.numeric(nb_pred), as.numeric(test_label))
  nbPerf <- performance(nbPredictiction, measure = "tpr", x.measure = "fpr")
  nbAUC <- performance(nbPredictiction, measure = "auc")
  
  print(plot(nbPerf))
  
  # Extract performance metrics
  sensitivity <- slot(nbPerf, "y.values")[[1]]
  specificity <- 1 - slot(nbPerf, "x.values")[[1]]
  auc <- slot(nbAUC, "y.values")
  nbError <- mean(as.numeric(nb_pred) !=as.numeric(test_label))
  
  # Print performance metrics
  print(nbConf)
  print(paste0("Sensitivity: ", sensitivity))
  print(paste0("Specificity: ", specificity))
  print(paste0("AUC: ", auc))
  print(paste0("Error rate:", nbError))
  
  # Calculate false positives
  false_positives <- sum(as.numeric(nb_pred) == 2 & as.numeric(test_label) == 1)
  
  # Calculate false positives as a percentage
  total_negatives <- sum(as.numeric(test_label) == 1)
  false_positives_percent <- false_positives / total_negatives * 100
  
  # Print the false positives percentage
  print(paste0("False positives percentage: ", round(false_positives_percent,3), "%"))
  
  return(list(false_positives_percent,round(acc,2)))
}
```


# MODELING

## Data Prep

```{r}
balanced.dummy.german <- ROSE(Default ~ ., data = newgerman, seed = 123)$data

RFE.4.standard <- RFE(newgerman)
RFE.4.balance <- RFE(balanced.dummy.german)
RFE.10.standard <- RFE(newgerman, num.features=10)
RFE.10.balance <- RFE(balanced.dummy.german, num.features=10)

PCA.4.standard <- Get.First.n.PCs(newgerman)
PCA.4.balance <- Get.First.n.PCs(balanced.dummy.german,4)
PCA.10.standard <- Get.First.n.PCs(newgerman,10)
PCA.10.balance <- Get.First.n.PCs(balanced.dummy.german,10)

```

## LDA

```{r}
lda.result <- LDA.Model(RFE.4.standard)
lda.result[]

lda.pca.result <- LDA.Model(PCA.4.standard)
lda.pca.result[]

lda.result2 <- LDA.Model(RFE.10.standard)
lda.result2[]

lda.pca.result2 <- LDA.Model(PCA.10.standard)
lda.pca.result2[]

lda.result3 <- LDA.Model(RFE.4.balance)
lda.result3[]

lda.pca.result3 <- LDA.Model(PCA.4.balance)
lda.pca.result3[]

lda.result4 <- LDA.Model(RFE.10.balance)
lda.result4[]

lda.pca.result4 <- LDA.Model(PCA.10.balance)
lda.pca.result4[]
```

## QDA

```{r}
qda.result <- QDA.Model(RFE.4.standard)
qda.result[]

qda.pca.result <- QDA.Model(PCA.4.standard)
qda.pca.result[]

qda.result2 <- QDA.Model(RFE.10.standard)
qda.result2[]

qda.pca.result2 <- QDA.Model(PCA.10.standard)
qda.pca.result2[]

qda.result3 <- QDA.Model(RFE.4.balance)
qda.result3[]

qda.pca.result3 <- QDA.Model(PCA.4.balance)
qda.pca.result3[]

qda.result4 <- QDA.Model(RFE.10.balance)
qda.result4[]

qda.pca.result4 <- QDA.Model(PCA.10.balance)
qda.pca.result4[]
```

## Logistic

```{r}
glm.result <- GLM.Model(RFE.4.standard)
glm.result[]

glm.pca.result <- GLM.Model(PCA.4.standard)
glm.pca.result[]

glm.result2 <- GLM.Model(RFE.10.standard)
glm.result2[]

glm.pca.result2 <- GLM.Model(PCA.10.standard)
glm.pca.result2[]

glm.result3 <- GLM.Model(RFE.4.balance)
glm.result3[]

glm.pca.result3 <- GLM.Model(PCA.4.balance)
glm.pca.result3[]

glm.result4 <- GLM.Model(RFE.10.balance)
glm.result4[]

glm.pca.result4 <- GLM.Model(PCA.10.balance)
glm.pca.result4[]
```

## KNN

```{r}
knn.result <- KNN.Model(RFE.4.standard)
knn.result[]

knn.pca.result <- KNN.Model(PCA.4.standard)
knn.pca.result[]

knn.result2 <- KNN.Model(RFE.10.standard)
knn.result2[]

knn.pca.result2 <- KNN.Model(PCA.10.standard)
knn.pca.result2[]

knn.result3 <- KNN.Model(RFE.4.balance)
knn.result3[]

knn.pca.result3 <- KNN.Model(PCA.4.balance)
knn.pca.result3[]

knn.result4 <- KNN.Model(RFE.10.balance)
knn.result4[]

knn.pca.result4 <- KNN.Model(PCA.10.balance)
knn.pca.result4[]
```

## Naïve Bayes

```{r}
nb.result <- NB.Model(RFE.4.standard)
nb.result[]

nb.pca.result <- NB.Model(PCA.4.standard)
nb.pca.result[]

nb.result2 <- NB.Model(RFE.10.standard)
nb.result2[]

nb.pca.result2 <- NB.Model(PCA.10.standard)
nb.pca.result2[]

nb.result3 <- NB.Model(RFE.4.balance)
nb.result3[]

nb.pca.result3 <- NB.Model(PCA.4.balance)
nb.pca.result3[]

nb.result4 <- NB.Model(RFE.10.balance)
nb.result4[]

nb.pca.result4 <- NB.Model(PCA.10.balance)
nb.pca.result4[]
```

# RESULTS

```{r}
comb_result1 <- cbind(RFE_4_feature = result[2],RFE_10_feature= result2[2],PC_5 = pca.result[2], PC_10 = pca.result2[2])

qda_comb_result1 <- cbind(RFE_4_feature = qda.result[2],RFE_10_feature= qda.result2[2],PC_5 = qda.pca.result[2], PC_10 = qda.pca.result2[2])

glm_comb_result1 <- cbind(RFE_4_feature = glm.result[2],RFE_10_feature= glm.result2[2],PC_5 = glm.pca.result[2], PC_10 = glm.pca.result2[2])

knn_comb_result1 <- cbind(RFE_4_feature = knn.result[2],RFE_10_feature= knn.result2[2],PC_5 = knn.pca.result[2], PC_10 = knn.pca.result2[2])

nb_comb_result1 <- cbind(RFE_4_feature = nb.result[2],RFE_10_feature= nb.result2[2],PC_5 = nb.pca.result[2], PC_10 = nb.pca.result2[2])

comb_result2 <- cbind(RFE_4_feature =result3[2],RFE_10_feature=result4[2],PC_5 = pca.result3[2], PC_10 = pca.result4[2])

qda_comb_result2 <- cbind(RFE_4_feature =qda.result3[2],RFE_10_feature=qda.result4[2],PC_5 = qda.pca.result3[2], PC_10 = qda.pca.result4[2])

glm_comb_result2 <- cbind(RFE_4_feature =glm.result3[2],RFE_10_feature=glm.result4[2],PC_5 = glm.pca.result3[2], PC_10 = glm.pca.result4[2])

knn_comb_result2 <- cbind(RFE_4_feature =knn.result3[2],RFE_10_feature=knn.result4[2],PC_5 = knn.pca.result3[2], PC_10 = knn.pca.result4[2])

nb_comb_result2 <- cbind(RFE_4_feature =nb.result3[2],RFE_10_feature=nb.result4[2],PC_5 = nb.pca.result3[2], PC_10 = nb.pca.result4[2])

comb_result <- rbind(comb_result1,qda_comb_result1,glm_comb_result1,knn_comb_result1, nb_comb_result1, comb_result2, qda_comb_result2, glm_comb_result2, knn_comb_result2, nb_comb_result2)


rownames(comb_result) <- c("LDA(std)","LDA(balance)","QDA(std)","QDA(balance)", "GLM(std)","GLM(balance)", "KNN(std)","KNN(balance)", "NB(std)","NB(balance)")
knitr::kable(comb_result, "simple", 
             caption = paste0("Model Accuracy (MSE) tested on ", 
                              ifelse(train==1, 100*test, (1-train)*100),
                              "% of the data"))

```

```{r}

comb_result1 <- cbind(RFE_4_feature = result[1],RFE_10_feature= result2[1],PC_5 = pca.result[1], PC_10 = pca.result2[1])

qda_comb_result1 <- cbind(RFE_4_feature = qda.result[1],RFE_10_feature= qda.result2[1],PC_5 = qda.pca.result[1], PC_10 = qda.pca.result2[1])

glm_comb_result1 <- cbind(RFE_4_feature = glm.result[1],RFE_10_feature= glm.result2[1],PC_5 = glm.pca.result[1], PC_10 = glm.pca.result2[1])

knn_comb_result1 <- cbind(RFE_4_feature = knn.result[1],RFE_10_feature= knn.result2[1],PC_5 = knn.pca.result[1], PC_10 = knn.pca.result2[1])

nb_comb_result1 <- cbind(RFE_4_feature = nb.result[1],RFE_10_feature= nb.result2[1],PC_5 = nb.pca.result[1], PC_10 = nb.pca.result2[1])

comb_result2 <- cbind(RFE_4_feature =result3[1],RFE_10_feature=result4[1],PC_5 = pca.result3[1], PC_10 = pca.result4[1])

qda_comb_result2 <- cbind(RFE_4_feature =qda.result3[1],RFE_10_feature=qda.result4[1],PC_5 = qda.pca.result3[1], PC_10 = qda.pca.result4[1])

glm_comb_result2 <- cbind(RFE_4_feature =glm.result3[1],RFE_10_feature=glm.result4[1],PC_5 = glm.pca.result3[1], PC_10 = glm.pca.result4[1])

knn_comb_result2 <- cbind(RFE_4_feature =knn.result3[1],RFE_10_feature=knn.result4[1],PC_5 = knn.pca.result3[1], PC_10 = knn.pca.result4[1])

nb_comb_result2 <- cbind(RFE_4_feature =nb.result3[1],RFE_10_feature=nb.result4[1],PC_5 = nb.pca.result3[1], PC_10 = nb.pca.result4[1])

comb_result <- rbind(comb_result1,qda_comb_result1,glm_comb_result1,knn_comb_result1, nb_comb_result1, comb_result2, qda_comb_result2, glm_comb_result2, knn_comb_result2, nb_comb_result2)

rownames(comb_result) <- c("LDA(std)","LDA(balance)","QDA(std)","QDA(balance)", "GLM(std)","GLM(balance)", "KNN(std)","KNN(balance)", "NB(std)","NB(balance)")

knitr::kable(comb_result, "rst",
             caption = "False Positive Rates")

comb_result
qda_comb_result1
```
